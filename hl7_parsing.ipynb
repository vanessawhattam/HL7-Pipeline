{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pyodbc\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# For the filename of the output file\n",
    "combined_df_date = datetime.now().strftime(\"%m%d%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the column names\n",
    "def sanitize_column_name(name):\n",
    "    return re.sub(r'[^a-zA-Z0-9_]', '_', name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process HL7 message correctly\n",
    "def process_hl7_messages(hl7_messages):\n",
    "    # Separate the segment header and field value \n",
    "    data = {'segment': [], 'field_value': []}\n",
    "    # Count the field number of the segment \n",
    "    segment_counter = {}\n",
    "\n",
    "    # From the list of hl7 messages\n",
    "    for message in hl7_messages:\n",
    "        # Split into segments by newlines '\\n'\n",
    "        segments = message.strip().split('\\n')  # Split the message into segments\n",
    "        for segment in segments:\n",
    "            if segment:\n",
    "                # Split our segment by the delimiter \"|\"\n",
    "                parts = segment.split('|')\n",
    "                if len(parts) > 1:  # Ensure the segment has values\n",
    "                    # Get the name of the segment\n",
    "                    segment_label, *segment_values = parts\n",
    "                    # Make the suffix the segment number\n",
    "                    suffix = segment_counter.get(segment_label, 0) + 1\n",
    "                    # Join the segment number to the segment label\n",
    "                    segment_counter[segment_label] = suffix\n",
    "                    value = '|'.join([str(suffix)] + segment_values)\n",
    "                    data['segment'].append(f\"{segment_label}-{suffix}\")\n",
    "                    data['field_value'].append(value)\n",
    "    # Put it all into a dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a file was modified within the last X hours\n",
    "def find_recently_modified_files(main_directory):\n",
    "    # Create a list to hold the file paths of the recently modified files\n",
    "    modified_files = []\n",
    "\n",
    "    # Iterate through all the subdirectories and files in the main directory\n",
    "    for root, dirs, files in os.walk(main_directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Get the modification time of the file\n",
    "            file_mtime = os.path.getmtime(file_path)\n",
    "\n",
    "            # Convert the modification time to a datetime object\n",
    "            file_datetime = datetime.fromtimestamp(file_mtime)\n",
    "\n",
    "            # Get the current date and time and make it retroactive X days\n",
    "            current_datetime = datetime.now() - timedelta(days=0)\n",
    "\n",
    "            # Calculate the datetime threshold for the last X days\n",
    "            threshold_datetime = current_datetime - timedelta(days=23)\n",
    "\n",
    "            # Check if the file was modified within the last X days\n",
    "            # If yes, then add to our modified_files list\n",
    "            if file_datetime >= threshold_datetime:\n",
    "                modified_files.append(file_path)\n",
    "\n",
    "    return modified_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date columns to datetime \n",
    "def convert_and_remove_datetime(value):\n",
    "    if value.isalpha():\n",
    "        return None\n",
    "    elif len(value) == 8:\n",
    "        value += \"0000\"\n",
    "    elif len(value) > 12:\n",
    "        value = value[:12]\n",
    "    return pd.to_datetime(value, format='%Y%m%d%H%M', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block identifies the valid extensions and determines which messages were sent within the last 24 hours\n",
    "# Input directory is where raw HL7s are stored\n",
    "input_directory = '//enthhs0112.state.mt.ads/MIDIS/Input_Files'\n",
    "\n",
    "# Filter files based on modification time\n",
    "main_directory = find_recently_modified_files(input_directory)\n",
    "\n",
    "# Don't include files in the 'Jennifer Uploads' files because don't want to duplicate\n",
    "main_directory = [x for x in main_directory if 'Jennifer Uploads' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688\n"
     ]
    }
   ],
   "source": [
    "# Now I want to create the conditions so I need to select each of the OBX-\\d.5, \n",
    "\n",
    "\n",
    "# Create a temporary directory\n",
    "# This will hold our files that we append the .txt file extension to\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Initialize an empty dataframe to store the data from the HL7s\n",
    "# Each message will be one row\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty dataframe to store errors\n",
    "errors_df = pd.DataFrame(columns=['file_path', 'error_message'])\n",
    "\n",
    "# Only read in files that have a .txt file extension\n",
    "valid_extensions = ['.txt']\n",
    "\n",
    "# Walk through all modified files\n",
    "for source_file in main_directory:\n",
    "    filename = os.path.basename(source_file)\n",
    "    # Get the new file name with the .txt extension\n",
    "    new_filename = os.path.splitext(filename)[0] + '.txt'\n",
    "    # Create the full path of the destination file in the temporary directory\n",
    "    destination_file = os.path.join(temp_dir, new_filename)\n",
    "    # Copy the source file to the temporary directory and change its extension to .txt\n",
    "    shutil.copyfile(source_file, destination_file)\n",
    "\n",
    "    # Check if the file has a valid file extension\n",
    "    if any(new_filename.endswith(ext) for ext in valid_extensions):\n",
    "        try:\n",
    "            # Read the file and split it into a list of messages based on MSH segment\n",
    "            # This is for batched messages where a file contains more than one message\n",
    "            with open(destination_file, 'r', errors='replace') as file:\n",
    "                hl7_messages_list = file.read().split('MSH')\n",
    "\n",
    "            # Process each HL7 message separately\n",
    "            for hl7_message in hl7_messages_list[1:]:  # Skip first empty entry \n",
    "                # Add back 'MSH' to each message and split on new line\n",
    "                hl7_message = 'MSH' + hl7_message\n",
    "                hl7_message_segments = hl7_message.split('\\n')\n",
    "                \n",
    "                # Use the process_hl7_messages to process each message\n",
    "                df = process_hl7_messages(hl7_message_segments)\n",
    "\n",
    "                # Set 'segment' as the index and transpose the dataframe\n",
    "                # We want the segments as the columns and each message to be a row\n",
    "                df = df.set_index('segment').transpose()\n",
    "\n",
    "                # Reset the index to make df concatenation easier\n",
    "                df.reset_index(inplace=True, drop=True)\n",
    "                \n",
    "                # Make all the columns string-type\n",
    "                # errored when we didn't do this\n",
    "                df = df.astype(str)\n",
    "\n",
    "                # Now we want to split the segments so each component is its own column\n",
    "                for column in df.columns:\n",
    "                    # Check if the column contains string values\n",
    "                    if df[column].dtype == 'O':\n",
    "                        # Split the values in the column using \"|\" as the delimiter\n",
    "                        split_columns = df[column].str.split('|', expand=True)\n",
    "\n",
    "                        # Add a separator \".\" to the column names\n",
    "                        split_columns.columns = split_columns.columns.map(lambda x: f'{column}.{x}')\n",
    "\n",
    "                        # Concatenate the new columns to the original DataFrame\n",
    "                        df = pd.concat([df, split_columns], axis=1)\n",
    "\n",
    "                        # Drop the original column\n",
    "                        df = df.drop(column, axis=1)\n",
    "\n",
    "                # Select the columns we want to concatenate in our final df\n",
    "                selected_columns = ['MSH-1.6',\n",
    "                                    'MSH-1.9', \n",
    "                                    'MSH-1.3', \n",
    "                                    'PID-1.22',\n",
    "                                    'PID-1.10',\n",
    "                                    'PID-1.5',\n",
    "                                    'PID-1.7',\n",
    "                                    'PID-1.8',\n",
    "                                    'PID-1.11',\n",
    "                                    'PID-1.13', \n",
    "                                    'PID-1.18', \n",
    "                                    'PV1-1.2', \n",
    "                                    'PV1-1.3', \n",
    "                                    'NTE-1.3',\n",
    "                                    'OBR-1.3',\n",
    "                                    'ORC-1.3'\n",
    "                                    ]\n",
    "\n",
    "                # Add OBX columns matching the pattern OBX-\\d.3, OBX-\\d.5, and OBX-\\d.14\n",
    "                obx_columns = [col for col in df.columns if re.match(r'OBX-\\d{1,2}\\.(3|5|14)$', col)]\n",
    "                selected_columns.extend(obx_columns)\n",
    "\n",
    "                # Get the columns present in df\n",
    "                existing_columns = df.columns.tolist()\n",
    "\n",
    "                # Identify if the dataframe is missing any of the selected_columns\n",
    "                missing_columns = [col for col in selected_columns if col not in existing_columns]\n",
    "\n",
    "                # If there are missing columns, create them and fill them with blanks\n",
    "                # Put them into a dataframe\n",
    "                blank_data = {col: [''] * len(df) for col in missing_columns}\n",
    "                blank_df = pd.DataFrame(blank_data)\n",
    "\n",
    "                # Merge the blank DataFrame with the df DataFrame\n",
    "                df = pd.concat([df, blank_df], axis=1)\n",
    "\n",
    "                # Reorder the columns to match the order of selected_columns\n",
    "                df = df[selected_columns]\n",
    "\n",
    "                # Reset index before concatenating to avoid non-unique index error\n",
    "                combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                # Combine the message-specific df with the overall df\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "        # Exceptions clause to add any errored messages to the errors df\n",
    "        except UnicodeDecodeError as e:\n",
    "            # Add the file path and the error message to the dataframe\n",
    "            errors_df = errors_df.append({'file_path': destination_file, 'error_message': str(e)}, ignore_index=True)\n",
    "            print(f\"Error processing file: {destination_file}. Error message: {str(e)}\")\n",
    "            continue\n",
    "        except FileNotFoundError as e:\n",
    "            # Add the file path and the error message to the dataframe\n",
    "            errors_df = errors_df.append({'file_path': destination_file, 'error_message': str(e)}, ignore_index=True)\n",
    "            print(f\"File not found: {destination_file}. Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Display how many rows the combined_df has\n",
    "print(len(combined_df))\n",
    "\n",
    "# After processing, remove the temporary directory\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "# Write the combined_df to a csv just to be safe\n",
    "combined_df.to_csv(f\"historic_combined_df/combined_df_thru_{combined_df_date}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "con = pyodbc.connect('DSN=SQL_Server_Connection')\n",
    "\n",
    "# Switch Database using SQL commands\n",
    "cursor = con.cursor()\n",
    "cursor.execute(\"USE ELRDQMS\")\n",
    "\n",
    "# Create a connection to SQL Server database using sqlalchemy\n",
    "engine = create_engine('mssql+pyodbc://', creator=lambda: con)\n",
    "\n",
    "# Define table name\n",
    "table_name = 'source_data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create SQL query to create table if needed\n",
    "columns = \", \".join([f\"{sanitize_column_name(col)} VARCHAR(MAX)\" for col in combined_df.columns]) \n",
    "\n",
    "# Write the table creation query\n",
    "create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({columns})\"\n",
    " \n",
    "# Execute the create table query\n",
    "cursor.execute(create_table_query)\n",
    " \n",
    "# Commit the table creation query\n",
    "con.commit()\n",
    "\n",
    "# Sanitize the column names to make the data match\n",
    "combined_df.columns = [sanitize_column_name(col) for col in combined_df.columns]\n",
    "\n",
    "# Insert DataFrame into SQL Server tabl0\n",
    "combined_df.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "\n",
    "# Delete any duplicates from the SQL Server table\n",
    "\n",
    "# Fetch column names from the table\n",
    "cursor.execute(f\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table_name}'\")\n",
    "\n",
    "columns = [row.COLUMN_NAME for row in cursor.fetchall()]\n",
    "\n",
    "# Generate criteria string using all columns\n",
    "criteria = ','.join(columns)\n",
    "\n",
    "# Execute SQL query to remove duplicate rows\n",
    "\n",
    "sql_query = f'''\n",
    "WITH CTE AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY {criteria} ORDER BY (SELECT NULL)) AS rn\n",
    "    FROM {table_name}\n",
    ")\n",
    "DELETE FROM CTE\n",
    "WHERE rn > 1;\n",
    "'''\n",
    "\n",
    "cursor.execute(sql_query)\n",
    "\n",
    "# Commit changes and close connection\n",
    "con.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "con.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9240d949b7e875368571ba59acc67192d2efbcc4561b3c6f94c83d7858e18732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
